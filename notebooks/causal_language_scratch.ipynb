{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05dab68a",
   "metadata": {},
   "source": [
    "# Training a causal language model from scratch\n",
    "\n",
    "Up until now, weâ€™ve mostly been using pretrained models and fine-tuning them for new use cases by reusing the weights from pretraining. As we saw in Chapter 1, this is commonly referred to as transfer learning, and itâ€™s a very successful strategy for applying Transformer models to most real-world use cases where labeled data is sparse. In this chapter, weâ€™ll take a different approach and train a completely new model from scratch. This is a good approach to take if you have a lot of data and it is very different from the pretraining data used for the available models. However, it also requires considerably more compute resources to pretrain a language model than just to fine-tune an existing one. Examples where it can make sense to train a new model include for datasets consisting of musical notes, molecular sequences such as DNA, or programming languages. The latter have recently gained traction thanks to tools such as TabNine and GitHubâ€™s Copilot, powered by OpenAIâ€™s Codex model, that can generate long sequences of code. This task of text generation is best addressed with auto-regressive or causal language models such as GPT-2.\n",
    "\n",
    "In this section we will build a scaled-down version of a code generation model: weâ€™ll focus on one-line completions instead of full functions or classes, using a subset of Python code. When working with data in Python you are in frequent contact with the Python data science stack, consisting of the matplotlib, seaborn, pandas, and scikit-learn libraries. When using those frameworks itâ€™s common to need to look up specific commands, so it would be nice if we could use a model to complete these calls for us.\n",
    "\n",
    "We created an efficient tokenizer to process Python source code, but what we still need is a large-scale dataset to pretrain a model on. Here, weâ€™ll apply our tokenizer to a corpus of Python code derived from GitHub repositories. We will then use the Trainer API and ðŸ¤— Accelerate to train the model. Letâ€™s get to it!\n",
    "\n",
    "## Gathering the data \n",
    "\n",
    "Python code is abundantly available from code repositories such as GitHub, which we can use to create a dataset by scraping for every Python repository. This was the approach taken in the Transformers textbook to pretrain a large GPT-2 model. Using a GitHub dump of about 180 GB containing roughly 20 million Python files called codeparrot, the authors built a dataset that they then shared on the Hugging Face Hub.\n",
    "\n",
    "However, training on the full corpus is time- and compute-consuming, and we only need the subset of the dataset concerned with the Python data science stack. So, letâ€™s start by filtering the codeparrot dataset for all files that include any of the libraries in this stack. Because of the datasetâ€™s size, we want to avoid downloading it; instead, weâ€™ll use the streaming feature to filter it on the fly. To help us filter the code samples using the libraries we mentioned earlier, weâ€™ll use the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58192161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def any_keyword_in_string(string, keywords):\n",
    "    for keyword in keywords:\n",
    "        if keyword in string:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "914d3234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False True\n"
     ]
    }
   ],
   "source": [
    "filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
    "example_1 = \"import numpy as np\"\n",
    "example_2 = \"import pandas as pd\"\n",
    "\n",
    "print(any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c0c603",
   "metadata": {},
   "source": [
    "We can use this to create a function that will stream the dataset and filter the elements we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b54d2f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from tqdm import tqdm\n",
    "\n",
    "def filter_streaming_dataset(dataset, filters):\n",
    "    filtered_dict = collections.defaultdict(list)\n",
    "    total = 0\n",
    "    for sample in tqdm(iter(dataset)):\n",
    "        total += 1\n",
    "        if any_keyword_in_string(sample[\"content\"], filters):\n",
    "            for k, v in sample.items():\n",
    "                filtered_dict[k].append(v)\n",
    "    print(f\"{len(filtered_dict['content'])/total:.2%} of data after filtering.\")\n",
    "    return Dataset.from_dict(filtered_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89705f92",
   "metadata": {},
   "source": [
    "Then we can simply apply this function to the streaming dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d351294",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# This cell is to fetch the data -- we will load the data directly\n",
    "\n",
    "# This cell will take a very long time to execute, so you should skip it and go to\n",
    "# the next one!\n",
    "from datasets import load_dataset\n",
    "\n",
    "split = \"train\"  # \"valid\"\n",
    "filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
    "\n",
    "data = load_dataset(f\"transformersbook/codeparrot-{split}\", split=split, streaming=True)\n",
    "filtered_data = filter_streaming_dataset(data, filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da679b8b",
   "metadata": {},
   "source": [
    "This leaves us with about 3% of the original dataset, which is still quite sizable â€” the resulting dataset is 6 GB and consists of 600,000 Python scripts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9a90b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration huggingface-course--codeparrot-ds-train-a9b1bc4c2b855d04\n",
      "Reusing dataset json (/Users/danilobustos/.cache/huggingface/datasets/json/huggingface-course--codeparrot-ds-train-a9b1bc4c2b855d04/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n",
      "Using custom data configuration huggingface-course--codeparrot-ds-valid-e5ece22bd7b6a6ac\n",
      "Reusing dataset json (/Users/danilobustos/.cache/huggingface/datasets/json/huggingface-course--codeparrot-ds-valid-e5ece22bd7b6a6ac/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 606720\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 3322\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
    "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_train,  # .shuffle().select(range(50000)),\n",
    "        \"valid\": ds_valid,  # .shuffle().select(range(500))\n",
    "    }\n",
    ")\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "528d9663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO_NAME: kmike/scikit-learn\n",
      "PATH: sklearn/utils/__init__.py\n",
      "COPIES: 3\n",
      "SIZE: 10094\n",
      "CONTENT: \"\"\"\n",
      "The :mod:`sklearn.utils` module includes various utilites.\n",
      "\"\"\"\n",
      "\n",
      "from collections import Sequence\n",
      "\n",
      "import numpy as np\n",
      "from scipy.sparse import issparse\n",
      "import warnings\n",
      "\n",
      "from .murmurhash import murm\n",
      "LICENSE: bsd-3-clause\n"
     ]
    }
   ],
   "source": [
    "for key in raw_datasets[\"train\"][0]:\n",
    "    print(f\"{key.upper()}: {raw_datasets['train'][0][key][:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b97351",
   "metadata": {},
   "source": [
    "We can see that the content field contains the code that we want our model to train on. Now that we have a dataset, we need to prepare the texts so theyâ€™re in a format suitable for pretraining.\n",
    "\n",
    "##  Preparing the dataset \n",
    "\n",
    "The first step will be to tokenize the data, so we can use it for training. Since our goal is to mainly autocomplete short function calls, we can keep the context size relatively small. This has the benefit that we can train the model much faster and it requires significantly less memory. If it is important for your application to have more context (for example, if you want the model to write unit tests based on a file with the function definition), make sure you increase that number, but also keep in mind that this comes with a greater GPU memory footprint. For now, letâ€™s fix the context size at 128 tokens, as opposed to the 1,024 or 2,048 used in GPT-2 or GPT-3, respectively.\n",
    "\n",
    "Most documents contain many more than 128 tokens, so simply truncating the inputs to the maximum length would eliminate a large fraction of our dataset. Instead, weâ€™ll use the return_overflowing_tokens option to tokenize the whole input and split it into several chunks, as we did in Chapter 6. Weâ€™ll also use the return_length option to return the length of each created chunk automatically. Often the last chunk will be smaller than the context size, and weâ€™ll get rid of these pieces to avoid padding issues; we donâ€™t really need them as we have plenty of data anyway.\n",
    "\n",
    "Letâ€™s see exactly how this works by looking at the first two examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "794d8324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs length: 34\n",
      "Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]\n",
      "Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "context_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
    "\n",
    "outputs = tokenizer(\n",
    "    raw_datasets[\"train\"][:2][\"content\"],\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True,\n",
    ")\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191eb3dc",
   "metadata": {},
   "source": [
    "We can see that we get 34 segments in total from those two examples. Looking at the chunk lengths, we can see that the chunks at the ends of both documents have less than 128 tokens (117 and 41, respectively). These represent just a small fraction of the total chunks that we have, so we can safely throw them away. With the overflow_to_sample_mapping field, we can also reconstruct which chunks belonged to which input samples.\n",
    "\n",
    "With this operation weâ€™re using a handy feature of the Dataset.map() function in ðŸ¤— Datasets, which is that it does not require one-to-one maps; as we saw in section 3, we can create batches with more or fewer elements than the input batch. This is useful when doing operations like data augmentation or data filtering that change the number of elements. In our case, when tokenizing each element into chunks of the specified context size, we create many samples from each document. We just need to make sure to delete the existing columns, since they have a conflicting size. If we wanted to keep them, we could repeat them appropriately and return them within the Dataset.map() call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "233f2171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/danilobustos/.cache/huggingface/datasets/json/huggingface-course--codeparrot-ds-train-a9b1bc4c2b855d04/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde/cache-0d1859607a8b0257.arrow\n",
      "Loading cached processed dataset at /Users/danilobustos/.cache/huggingface/datasets/json/huggingface-course--codeparrot-ds-valid-e5ece22bd7b6a6ac/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde/cache-8573d3e1f22381a4.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 16702061\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 93164\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c01967",
   "metadata": {},
   "source": [
    "We now have 16.7 million examples with 128 tokens each, which corresponds to about 2.1 billion tokens in total. For reference, OpenAIâ€™s GPT-3 and Codex models are trained on 300 and 100 billion tokens, respectively, where the Codex models are initialized from the GPT-3 checkpoints. Our goal in this section is not to compete with these models, which can generate long, coherent texts, but to create a scaled-down version providing a quick autocomplete function for data scientists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7dd2cf",
   "metadata": {},
   "source": [
    "##  Initializing a new model \n",
    "\n",
    "Our first step is to freshly initialize a GPT-2 model. Weâ€™ll use the same configuration for our model as for the small GPT-2 model, so we load the pretrained configuration, make sure that the tokenizer size matches the model vocabulary size and pass the bos and eos (beginning and end of sequence) token IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0112c4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fec79d",
   "metadata": {},
   "source": [
    "With that configuration, we can load a new model. Note that this is the first time we donâ€™t use the from_pretrained() function, since weâ€™re actually initializing a model ourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "206c4e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 124.2M parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05696ca",
   "metadata": {},
   "source": [
    "Our model has 124M parameters that weâ€™ll have to tune. Before we can start training, we need to set up a data collator that will take care of creating the batches. We can use the DataCollatorForLanguageModeling collator, which is designed specifically for language modeling (as the name subtly suggests). Besides stacking and padding batches, it also takes care of creating the language model labels â€” in **causal language modeling the inputs serve as labels too (just shifted by one element)**, and this data collator creates them on the fly during training so we donâ€™t need to duplicate the input_ids.\n",
    "\n",
    "**Note that DataCollatorForLanguageModeling supports both masked language modeling (MLM) and causal language modeling (CLM). By default it prepares data for MLM, but we can switch to CLM by setting the argument mlm=False:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8191854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5016d990",
   "metadata": {},
   "source": [
    "Letâ€™s have a look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6deb3ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([5, 128])\n",
      "attention_mask shape: torch.Size([5, 128])\n",
      "labels shape: torch.Size([5, 128])\n"
     ]
    }
   ],
   "source": [
    "out = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1b2738",
   "metadata": {},
   "source": [
    "Now we have everything in place to actually train our model â€” that wasnâ€™t so much work after all! Before we start training we should log in to Hugging Face. If youâ€™re working in a notebook, you can do so with the following utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "349ed81a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12425e20f1b2436d8810aa91fb4a9362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center>\\n<img src=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db0ebe2",
   "metadata": {},
   "source": [
    "All thatâ€™s left to do is configure the training arguments and fire up the Trainer. Weâ€™ll use a cosine learning rate schedule with some warmup and an effective batch size of 256 (per_device_train_batch_size * gradient_accumulation_steps). Gradient accumulation is used when a single batch does not fit into memory, and incrementally builds up the gradient through several forward/backward passes. Weâ€™ll see this in action when we create the training loop with ðŸ¤— Accelerate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8eb5953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilobustos/transformers/notebooks/codeparrot-ds is already a clone of https://huggingface.co/dbustosp/codeparrot-ds. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"codeparrot-ds\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    fp16=False,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6e2971",
   "metadata": {},
   "source": [
    "Now we can just start the Trainer and wait for training to finish. Depending on whether you run it on the full or a subset of the training set this will take 20 or 2 hours, respectively, so grab a few coffees and a good book to read!\n",
    "\n",
    " If you have access to a machine with multiple GPUs, try to run the code there. The Trainer automatically manages multiple machines, and this can speed up training tremendously.\n",
    " \n",
    " ##  Code generation with a pipeline \n",
    " \n",
    " Now is the moment of truth: letâ€™s see how well the trained model actually works! We can see in the logs that the loss went down steadily, but to put the model to the test letâ€™s take a look at how well it works on some prompts. To do that weâ€™ll wrap the model in a text generation pipeline, and weâ€™ll put it on the GPU for fast generations if there is one available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "612aee87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97cb8076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/huggingface-course/codeparrot-ds/resolve/main/config.json not found in cache or force_download set to True, downloading to /Users/danilobustos/.cache/huggingface/transformers/tmpzhboh2x_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee1b3b0d16841fd8c40f3988dcda2af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/938 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/huggingface-course/codeparrot-ds/resolve/main/config.json in cache at /Users/danilobustos/.cache/huggingface/transformers/e8be2e6a353dfe3f0c0b18c02b8971c89f535483cb39e6ebbf7a08bae147974e.447dba843fb4bf02bbeb8b8233e311b50af0181eafad0fe0f2300209522ab44d\n",
      "creating metadata file for /Users/danilobustos/.cache/huggingface/transformers/e8be2e6a353dfe3f0c0b18c02b8971c89f535483cb39e6ebbf7a08bae147974e.447dba843fb4bf02bbeb8b8233e311b50af0181eafad0fe0f2300209522ab44d\n",
      "loading configuration file https://huggingface.co/huggingface-course/codeparrot-ds/resolve/main/config.json from cache at /Users/danilobustos/.cache/huggingface/transformers/e8be2e6a353dfe3f0c0b18c02b8971c89f535483cb39e6ebbf7a08bae147974e.447dba843fb4bf02bbeb8b8233e311b50af0181eafad0fe0f2300209522ab44d\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"./models/codeparrot-ds/checkpoint-55000/\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": true,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_length\": 50,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 128,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "https://huggingface.co/huggingface-course/codeparrot-ds/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /Users/danilobustos/.cache/huggingface/transformers/tmpzul75xtu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac81bcb028d40ab8ef9fc93f0a19861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/486M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/huggingface-course/codeparrot-ds/resolve/main/pytorch_model.bin in cache at /Users/danilobustos/.cache/huggingface/transformers/7028996ad2ab0758d295841c7dbba21d490af098593398d67b1e57d0dc1f1215.039574312d0fcaef2c6c5f13798143febdee6899dc526124c4f7bc55e26cef07\n",
      "creating metadata file for /Users/danilobustos/.cache/huggingface/transformers/7028996ad2ab0758d295841c7dbba21d490af098593398d67b1e57d0dc1f1215.039574312d0fcaef2c6c5f13798143febdee6899dc526124c4f7bc55e26cef07\n",
      "loading weights file https://huggingface.co/huggingface-course/codeparrot-ds/resolve/main/pytorch_model.bin from cache at /Users/danilobustos/.cache/huggingface/transformers/7028996ad2ab0758d295841c7dbba21d490af098593398d67b1e57d0dc1f1215.039574312d0fcaef2c6c5f13798143febdee6899dc526124c4f7bc55e26cef07\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at huggingface-course/codeparrot-ds.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('huggingface-course/codeparrot-ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5e5ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29b24195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/huggingface-course/codeparrot-ds/resolve/main/config.json from cache at /Users/danilobustos/.cache/huggingface/transformers/e8be2e6a353dfe3f0c0b18c02b8971c89f535483cb39e6ebbf7a08bae147974e.447dba843fb4bf02bbeb8b8233e311b50af0181eafad0fe0f2300209522ab44d\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"huggingface-course/codeparrot-ds\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": true,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_length\": 50,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 128,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/huggingface-course/codeparrot-ds/resolve/main/config.json from cache at /Users/danilobustos/.cache/huggingface/transformers/e8be2e6a353dfe3f0c0b18c02b8971c89f535483cb39e6ebbf7a08bae147974e.447dba843fb4bf02bbeb8b8233e311b50af0181eafad0fe0f2300209522ab44d\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"huggingface-course/codeparrot-ds\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": true,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_length\": 50,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 128,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/huggingface-course/codeparrot-ds/resolve/main/pytorch_model.bin from cache at /Users/danilobustos/.cache/huggingface/transformers/7028996ad2ab0758d295841c7dbba21d490af098593398d67b1e57d0dc1f1215.039574312d0fcaef2c6c5f13798143febdee6899dc526124c4f7bc55e26cef07\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at huggingface-course/codeparrot-ds.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading file https://huggingface.co/huggingface-course/codeparrot-ds/resolve/main/vocab.json from cache at /Users/danilobustos/.cache/huggingface/transformers/6f55d52290449f76362027099c9061a04dc746bb06fd1e10c7a69efebfaacc4b.4fb676c48a3f16a72a79c0e191e5b5087d3c100b9d6672b960d958c09ec83eb6\n",
      "loading file https://huggingface.co/huggingface-course/codeparrot-ds/resolve/main/merges.txt from cache at /Users/danilobustos/.cache/huggingface/transformers/032357a797b07f8748e622fb7ad323b1f0eddcd700dbd66258cdf32f4b688089.2cde5d5ec9a675c75002950f79737dfc28bb37f2971280eea89af91188acd1b5\n",
      "loading file https://huggingface.co/huggingface-course/codeparrot-ds/resolve/main/tokenizer.json from cache at /Users/danilobustos/.cache/huggingface/transformers/8d0f3b28bd674d251cb6aba32a466f177d09428ec955136d52c157d356f89dcf.461f202cbe33855b980574fcef2db1b5fe8e5a242ed613aab2eb201268d29274\n",
      "loading file https://huggingface.co/huggingface-course/codeparrot-ds/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/huggingface-course/codeparrot-ds/resolve/main/special_tokens_map.json from cache at /Users/danilobustos/.cache/huggingface/transformers/10856b982a328cbbd2ecea1d3ac23d1ff53ce5cc12572da40eef7b1be432f5e9.3ae9ae72462581d20e36bc528e9c47bb30cd671bb21add40ca0b24a0be9fac22\n",
      "loading file https://huggingface.co/huggingface-course/codeparrot-ds/resolve/main/tokenizer_config.json from cache at /Users/danilobustos/.cache/huggingface/transformers/2e96f049cb47bcf38a30ea1e9f5bbb1744f10bba93c75bed15a5cb4df5603dc0.e1298507f79c9dcfbd22ce63a108d0dc0d59067de9b8efed7c4350a6d22d8f6f\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"huggingface-course/codeparrot-ds\", device=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c52bb71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# create some data\n",
      "x = np.random.randn(100)\n",
      "y = np.random.randn(100)\n",
      "\n",
      "# create scatter plot with x, y\n",
      "plt.scatter(x, y)\n",
      "\n",
      "\n",
      "# ### Add\n"
     ]
    }
   ],
   "source": [
    "txt = \"\"\"\\\n",
    "# create some data\n",
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "# create scatter plot with x, y\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87e2e944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# create some data\n",
      "x = np.random.randn(100)\n",
      "y = np.random.randn(100)\n",
      "\n",
      "# create dataframe from x and y\n",
      "df = pd.DataFrame(x, columns=['var%d'\n"
     ]
    }
   ],
   "source": [
    "txt = \"\"\"\\\n",
    "# create some data\n",
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "# create dataframe from x and y\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5cd7a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# dataframe with profession, income and name\n",
      "df = pd.DataFrame({'profession': x, 'income':y, 'name': z})\n",
      "\n",
      "# calculate the mean income per profession\n",
      "df.loc[:, '\n"
     ]
    }
   ],
   "source": [
    "txt = \"\"\"\\\n",
    "# dataframe with profession, income and name\n",
    "df = pd.DataFrame({'profession': x, 'income':y, 'name': z})\n",
    "\n",
    "# calculate the mean income per profession\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84f74f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# import random forest regressor from scikit-learn\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "# fit random forest model with 300 estimators on X, y:\n",
      "classifier = RandomTreesRegressor()\n",
      "classifier.fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "txt = \"\"\"\n",
    "# import random forest regressor from scikit-learn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# fit random forest model with 300 estimators on X, y:\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbc177a",
   "metadata": {},
   "source": [
    "Looking at these few examples, it seems that the model has learned some of the syntax of the Python data science stack (of course, we would need to evaluate it more thoroughly before deploying the model in the real world). Sometimes it requires more customization of the model training to achieve the necessary performance for a given use case, however. For example, what if we would like to dynamically update the batch size or have a conditional training loop that skips bad examples on the fly? One option would be to subclass the Trainer and add the necessary changes, but sometimes itâ€™s simpler to write the training loop from scratch. Thatâ€™s where ðŸ¤— Accelerate comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed20c1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b45a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9609c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ad4b12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
