{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b7317bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['books', 'electronics', 'grocery', 'movies', 'restaurants', 'tripadvisor']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "\n",
    "domains = get_dataset_config_names(\"subjqa\")\n",
    "domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74470d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset subjqa (/Users/danilobustos/.cache/huggingface/datasets/subjqa/electronics/1.1.0/e5588f9298ff2d70686a00cc377e4bdccf4e32287459e3c6baf2dc5ab57fe7fd)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4dd79ea14c4a3faf0e0b4a2e85d394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "subjqa = load_dataset(\"subjqa\", name=\"electronics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84c961cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Bass is weak as expected', 'Bass is weak as expected, even with EQ adjusted up'], 'answer_start': [1302, 1302], 'answer_subj_level': [1, 1], 'ans_subj_score': [0.5083333253860474, 0.5083333253860474], 'is_ans_subjective': [True, True]}\n"
     ]
    }
   ],
   "source": [
    "print(subjqa[\"train\"][\"answers\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fbe50e",
   "metadata": {},
   "source": [
    "We can see that the answers are stored in a text field, while the starting character indices are provided in answer_start. \n",
    "\n",
    "To explore the dataset more easily, \n",
    "we’ll flatten these nested columns with the flatten() \n",
    "method and convert each split to a Pandas DataFrame as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50b0a4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions in train: 1295\n",
      "Number of questions in test: 358\n",
      "Number of questions in validation: 255\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dfs = {split: dset.to_pandas() for split, dset in subjqa.flatten().items()}\n",
    "\n",
    "for split, df in dfs.items():\n",
    "    print(f\"Number of questions in {split}: {df['id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43229aa3",
   "metadata": {},
   "source": [
    "Column name\tDescription\n",
    "\n",
    "- title: The Amazon Standard Identification Number (ASIN) associated with each product\n",
    "- question: The question\n",
    "- answers.answer_text: The span of text in the review labeled by the annotator\n",
    "- answers.answer_start: The start character index of the answer span\n",
    "- context: The customer review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9132e983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>question</th>\n",
       "      <th>answers.text</th>\n",
       "      <th>answers.answer_start</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>B005DKZTMG</td>\n",
       "      <td>Does the keyboard lightweight?</td>\n",
       "      <td>[this keyboard is compact]</td>\n",
       "      <td>[215]</td>\n",
       "      <td>I really like this keyboard.  I give it 4 star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>B00AAIPT76</td>\n",
       "      <td>How is the battery?</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>I bought this after the first spare gopro batt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           title                        question                answers.text  \\\n",
       "791   B005DKZTMG  Does the keyboard lightweight?  [this keyboard is compact]   \n",
       "1159  B00AAIPT76             How is the battery?                          []   \n",
       "\n",
       "     answers.answer_start                                            context  \n",
       "791                 [215]  I really like this keyboard.  I give it 4 star...  \n",
       "1159                   []  I bought this after the first spare gopro batt...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_cols = [\"title\", \"question\", \"answers.text\", \"answers.answer_start\", \"context\"]\n",
    "sample_df = dfs[\"train\"][qa_cols].sample(2, random_state=7)\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f43e9b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I really like this keyboard.  I give it 4 stars because it doesn't have a CAPS LOCK key so I never know if my caps are on.  But for the price, it really suffices as a wireless keyboard.  I have very large hands and this keyboard is compact, but I have no complaints.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df[\"context\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a5cbcc",
   "metadata": {},
   "source": [
    "From these examples we can make a few observations. First, the questions are not grammatically correct, which is quite common in the FAQ sections of ecommerce websites. Second, an empty answers.text entry denotes “unanswerable” questions whose answer cannot be found in the review. Finally, we can use the start index and length of the answer span to slice out the span of text in the review that corresponds to the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba3af8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this keyboard is compact'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_idx = sample_df[\"answers.answer_start\"].iloc[0][0]\n",
    "end_idx = start_idx + len(sample_df[\"answers.text\"].iloc[0][0])\n",
    "sample_df[\"context\"].iloc[0][start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39a2e34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "counts = {}\n",
    "question_types = [\"What\", \"How\", \"Is\", \"Does\", \"Do\", \"Was\", \"Where\", \"Why\"]\n",
    "\n",
    "for q in question_types:\n",
    "    counts[q] = dfs[\"train\"][\"question\"].str.startswith(q).value_counts()[True]\n",
    "\n",
    "pd.Series(counts).sort_values().plot.barh()\n",
    "plt.title(\"Frequency of Question Types\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ad25429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How is the camera?\n",
      "How do you like the control?\n",
      "How fast is the charger?\n",
      "What is direction?\n",
      "What is the quality of the construction of the bag?\n",
      "What is your impression of the product?\n",
      "Is this how zoom works?\n",
      "Is sound clear?\n",
      "Is it a wireless keyboard?\n"
     ]
    }
   ],
   "source": [
    "for question_type in [\"How\", \"What\", \"Is\"]:\n",
    "    for question in (\n",
    "        dfs[\"train\"][dfs[\"train\"].question.str.startswith(question_type)]\n",
    "        .sample(n=3, random_state=42)['question']):\n",
    "        print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85e456a",
   "metadata": {},
   "source": [
    "## Extracting Answers from Text\n",
    "\n",
    "The first thing we’ll need for our QA system is to find a way to identify a potential answer as a span of text in a customer review. For example, if a we have a question like “Is it waterproof?” and the review passage is “This watch is waterproof at 30m depth”, then the model should output “waterproof at 30m”. To do this we’ll need to understand how to:\n",
    "\n",
    "- Frame the supervised learning problem.\n",
    "- Tokenize and encode text for QA tasks.\n",
    "- Deal with long passages that exceed a model’s maximum context size.\n",
    "\n",
    "Let’s start by taking a look at how to frame the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728290ea",
   "metadata": {},
   "source": [
    "### Span classification\n",
    "\n",
    "The most common way to extract answers from text is by framing the problem as a span classification task, where the start and end tokens of an answer span act as the labels that a model needs to predict\n",
    "\n",
    "As usual, the first thing we need is a tokenizer to encode our texts, so let’s take a look at how this works for QA tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4acd32",
   "metadata": {},
   "source": [
    "## Tokenizing text for QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcd0cb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"deepset/minilm-uncased-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eae93276",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How much music can this hold?\"\n",
    "context = \"\"\"An MP3 is about 1 MB/minute, so about 6000 hours depending on file size.\"\"\"\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8966d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2129,  2172,  2189,  2064,  2023,  2907,  1029,   102,  2019,\n",
       "         23378,  2003,  2055,  1015, 16914,  1013,  3371,  1010,  2061,  2055,\n",
       "         25961,  2847,  5834,  2006,  5371,  2946,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca9eaa2",
   "metadata": {},
   "source": [
    "To understand how the tokenizer formats the inputs for QA tasks, let’s decode the input_ids tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f3a5bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] how much music can this hold? [SEP] an mp3 is about 1 mb / minute, so about 6000 hours depending on file size. [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(inputs[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a8f889",
   "metadata": {},
   "source": [
    "where the location of the first [SEP] token is determined by the token_type_ids. Now that our text is tokenized, we just need to instantiate the model with a QA head and run the inputs through the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcc65635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-0.9862, -4.7750, -5.4025, -5.2378, -5.2863, -5.5117, -4.9819, -6.1880,\n",
      "         -0.9862,  0.2596, -0.2144, -1.7136,  3.7806,  4.8561, -1.0546, -3.9097,\n",
      "         -1.7374, -4.5944, -1.4278,  3.9949,  5.0391, -0.2018, -3.0193, -4.8549,\n",
      "         -2.3107, -3.5110, -3.5713, -0.9862]]), end_logits=tensor([[-0.9623, -5.4733, -5.0326, -5.1639, -5.4278, -5.5151, -5.1749, -4.6233,\n",
      "         -0.9623, -3.7855, -0.8715, -3.7745, -3.0161, -1.1780,  0.1758, -2.7365,\n",
      "          4.8934,  0.3046, -3.1761, -3.2762,  0.8937,  5.6606, -0.3623, -4.9554,\n",
      "         -3.2531, -0.0914,  1.6211, -0.9623]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe82dbe",
   "metadata": {},
   "source": [
    "Here we can see that we get a QuestionAnsweringModelOutput object as the output of the QA head. As illustrated in Figure 7-4, the QA head corresponds to a linear layer that takes the hidden states from the encoder and computes the logits for the start and end spans.10 This means that we treat QA as a form of token classification, similar to what we encountered for named entity recognition in Chapter 4. To convert the outputs into an answer span, we first need to get the logits for the start and end tokens:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0c7be3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c215c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([1, 28])\n",
      "Start logits shape: torch.Size([1, 28])\n",
      "End logits shape: torch.Size([1, 28])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input IDs shape: {inputs.input_ids.size()}\")\n",
    "print(f\"Start logits shape: {start_logits.size()}\")\n",
    "print(f\"End logits shape: {end_logits.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7b86fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How much music can this hold?\n",
      "Answer: 6000 hours\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "start_idx = torch.argmax(start_logits)\n",
    "end_idx = torch.argmax(end_logits) + 1\n",
    "answer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
    "answer = tokenizer.decode(answer_span)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaec622",
   "metadata": {},
   "source": [
    "Great, it worked! In Transformers, all of these preprocessing and postprocessing steps are conveniently wrapped in a dedicated pipeline. We can instantiate the pipeline by passing our tokenizer and fine-tuned model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1008428b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilobustos/miniforge3/envs/tensorflow_m1/lib/python3.9/site-packages/transformers/pipelines/question_answering.py:189: UserWarning: topk parameter is deprecated, use top_k instead\n",
      "  warnings.warn(\"topk parameter is deprecated, use top_k instead\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.26516157388687134,\n",
       "  'start': 38,\n",
       "  'end': 48,\n",
       "  'answer': '6000 hours'},\n",
       " {'score': 0.22082968056201935,\n",
       "  'start': 16,\n",
       "  'end': 48,\n",
       "  'answer': '1 MB/minute, so about 6000 hours'},\n",
       " {'score': 0.10253528505563736,\n",
       "  'start': 16,\n",
       "  'end': 27,\n",
       "  'answer': '1 MB/minute'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "pipe(question=question, context=context, topk=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0c9c6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9068412780761719, 'start': 0, 'end': 0, 'answer': ''}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(question=\"Why is there no data?\", context=context, handle_impossible_answer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b17099ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An MP3 is about 1 MB/minute, so about 6000 hours depending on file size.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314d5e24",
   "metadata": {},
   "source": [
    "NOTE\n",
    "In our simple example, we obtained the start and end indices by taking the argmax of the corresponding logits. However, this heuristic can produce out-of-scope answers by selecting tokens that belong to the question instead of the context. In practice, the pipeline computes the best combination of start and end indices subject to various constraints such as being in-scope, requiring the start indices to precede the end indices, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203a709e",
   "metadata": {},
   "source": [
    "## Dealing with long passages\n",
    "\n",
    "In Transformers, we can set return_overflowing_tokens=True in the tokenizer to enable the sliding window. The size of the sliding window is controlled by the max_seq_length argument, and the size of the stride is controlled by doc_stride. Let’s grab the first example from our training set and define a small window to illustrate how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a9050d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "example = dfs[\"train\"].iloc[0][[\"question\", \"context\"]]\n",
    "tokenized_example = tokenizer(example[\"question\"], example[\"context\"],\n",
    "                              return_overflowing_tokens=True, max_length=100,\n",
    "                              stride=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db6f2c1",
   "metadata": {},
   "source": [
    "In this case we now get a list of input_ids, one for each window. Let’s check the number of tokens we have in each window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bbb991a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window #0 has 100 tokens\n",
      "Window #1 has 88 tokens\n"
     ]
    }
   ],
   "source": [
    "for idx, window in enumerate(tokenized_example[\"input_ids\"]):\n",
    "    print(f\"Window #{idx} has {len(window)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71155506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] how is the bass? [SEP] i have had koss headphones in the past, pro 4aa and qz - 99. the koss portapro is portable and has great bass response. the work great with my android phone and can be \" rolled up \" to be carried in my motorcycle jacket or computer bag without getting crunched. they are very light and don't feel heavy or bear down on your ears even after listening to music with them on all day. the sound is [SEP] \n",
      "\n",
      "[CLS] how is the bass? [SEP] and don't feel heavy or bear down on your ears even after listening to music with them on all day. the sound is night and day better than any ear - bud could be and are almost as good as the pro 4aa. they are \" open air \" headphones so you cannot match the bass to the sealed types, but it comes close. for $ 32, you cannot go wrong. [SEP] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for window in tokenized_example[\"input_ids\"]:\n",
    "    print(f\"{tokenizer.decode(window)} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6914005b",
   "metadata": {},
   "source": [
    "Now that we have some intuition about how QA models can extract answers from text, let’s look at the other components we need to build an end-to-end QA pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4a080e",
   "metadata": {},
   "source": [
    "## Using Haystack to Build a QA Pipeline\n",
    "\n",
    "In our simple answer extraction example, we provided both the question and the context to the model. However, in reality our system’s users will only provide a question about a product, so we need some way of selecting relevant passages from among all the reviews in our corpus. One way to do this would be to concatenate all the reviews of a given product together and feed them to the model as a single, long context. Although simple, the drawback of this approach is that the context can become extremely long and thereby introduce an unacceptable latency for our users’ queries. For example, let’s suppose that on average, each product has 30 reviews and each review takes 100 milliseconds to process. If we need to process all the reviews to get an answer, this would result in an average latency of 3 seconds per user query—much too long for ecommerce websites!\n",
    "\n",
    "To handle this, modern QA systems are typically based on the retriever-reader architecture, which has two main components:\n",
    "\n",
    "### Retriever\n",
    "\n",
    "Responsible for retrieving relevant documents for a given query. Retrievers are usually categorized as sparse or dense. Sparse retrievers use word frequencies to represent each document and query as a sparse vector.11 The relevance of a query and a document is then determined by computing an inner product of the vectors. On the other hand, dense retrievers use encoders like transformers to represent the query and document as contextualized embeddings (which are dense vectors). These embeddings encode semantic meaning, and allow dense retrievers to improve search accuracy by understanding the content of the query.\n",
    "\n",
    "### Reader\n",
    "\n",
    "Responsible for extracting an answer from the documents provided by the retriever. The reader is usually a reading comprehension model, although at the end of the chapter we’ll see examples of models that can generate free-form answers.\n",
    "\n",
    "\n",
    "To build our QA system, we’ll use the [Haystack library](https://haystack.deepset.ai/overview/intro)\n",
    "\n",
    "\n",
    "In addition to the retriever and reader, there are two more components involved when building a QA pipeline with Haystack:\n",
    "\n",
    "**Document store**\n",
    "A document-oriented database that stores documents and metadata which are provided to the retriever at query time\n",
    "\n",
    "**Pipeline**\n",
    "Combines all the components of a QA system to enable custom query flows, merging documents from multiple retrievers, and more\n",
    "\n",
    "In this section we’ll look at how we can use these components to quickly build a prototype QA pipeline. Later, we’ll examine how we can improve its performance.\n",
    "\n",
    "## Initializing a document store\n",
    "\n",
    "In Haystack, there are various document stores to choose from and each one can be paired with a dedicated set of retrievers. This is illustrated in Table 7-3, where the compatibility of sparse (TF-IDF, BM25) and dense (Embedding, DPR) retrievers is shown for each of the available document stores.\n",
    "\n",
    "Since we’ll be exploring both sparse and dense retrievers in this chapter, we’ll use the ElasticsearchDocumentStore, which is compatible with both retriever types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb5ea87",
   "metadata": {},
   "source": [
    "# Elastic\n",
    "\n",
    "In a terminal: \n",
    "\n",
    "```\n",
    "~/infra/elasticsearch-7.17.0/bin/elasticsearch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ae4c883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "{\n",
      "  \"name\" : \"Danilos-MacBook-Pro.local\",\n",
      "  \"cluster_name\" : \"elasticsearch\",\n",
      "  \"cluster_uuid\" : \"GvlFMWICTr6wJUjTETUcBA\",\n",
      "  \"version\" : {\n",
      "    \"number\" : \"7.17.0\",\n",
      "    \"build_flavor\" : \"default\",\n",
      "    \"build_type\" : \"tar\",\n",
      "    \"build_hash\" : \"bee86328705acaa9a6daede7140defd4d9ec56bd\",\n",
      "    \"build_date\" : \"2022-01-28T08:36:04.875279988Z\",\n",
      "    \"build_snapshot\" : false,\n",
      "    \"lucene_version\" : \"8.11.1\",\n",
      "    \"minimum_wire_compatibility_version\" : \"6.8.0\",\n",
      "    \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n",
      "  },\n",
      "  \"tagline\" : \"You Know, for Search\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl -X GET \"localhost:9200/?pretty\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825f1432",
   "metadata": {},
   "source": [
    "Now that our Elasticsearch server is up and running, the next thing to do is instantiate the document store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aca6c315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilobustos/miniforge3/envs/tensorflow_m1/lib/python3.9/site-packages/elasticsearch/connection/base.py:190: ElasticsearchDeprecationWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "\n",
    "# Return the document embedding for later use with dense retriever\n",
    "document_store = ElasticsearchDocumentStore(return_embedding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a574e06",
   "metadata": {},
   "source": [
    "By default, `ElasticsearchDocumentStore` creates two indices on Elasticsearch: one called document for (you guessed it) storing documents, and another called label for storing the annotated answer spans. For now, we’ll just populate the document index with the SubjQA reviews, and Haystack’s document stores expect a list of dictionaries with text and meta keys as follows:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"text\": \"<the-context>\",\n",
    "    \"meta\": {\n",
    "        \"field_01\": \"<additional-metadata>\",\n",
    "        \"field_02\": \"<additional-metadata>\",\n",
    "        ...\n",
    "    }\n",
    "}```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f4fd72",
   "metadata": {},
   "source": [
    "The fields in `meta` can be used for applying filters during retrieval. For our purposes we’ll include the `item_id` and `q_review_id` columns of SubjQA so we can filter by product and question ID, along with the corresponding training split. We can then loop through the examples in each DataFrame and add them to the index with the `write_documents()` method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d639218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1615 documents\n"
     ]
    }
   ],
   "source": [
    "for split, df in dfs.items():\n",
    "    # Exclude duplicate reviews\n",
    "    docs = [{\"content\": row[\"context\"],\n",
    "             \"meta\":{\"item_id\": row[\"title\"], \"question_id\": row[\"id\"],\n",
    "                     \"split\": split}}\n",
    "        for _,row in df.drop_duplicates(subset=\"context\").iterrows()]\n",
    "    document_store.write_documents(docs, index=\"document\")\n",
    "\n",
    "print(f\"Loaded {document_store.get_document_count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7337f63b",
   "metadata": {},
   "source": [
    "Great, we’ve loaded all our reviews into an index! To search the index we’ll need a retriever, so let’s look at how we can initialize one for Elasticsearch.\n",
    "\n",
    "## Initializing a retriever\n",
    "\n",
    "The Elasticsearch document store can be paired with any of the Haystack retrievers, so let’s start by using a sparse retriever based on BM25 (short for “Best Match 25”). BM25 is an improved version of the classic Term Frequency-Inverse Document Frequency (TF-IDF) algorithm and represents the question and context as sparse vectors that can be searched efficiently on Elasticsearch. The BM25 score measures how much matched text is about a search query and improves on TF-IDF by saturating TF values quickly and normalizing the document length so that short documents are favored over long ones.\n",
    "\n",
    "In Haystack, the BM25 retriever is used by default in `ElasticsearchRetriever`, so let’s initialize this class by specifying the document store we wish to search over:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "629ee505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.retriever.sparse import ElasticsearchRetriever\n",
    "\n",
    "es_retriever = ElasticsearchRetriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2a0af3",
   "metadata": {},
   "source": [
    "Next, let’s look at a simple query for a single electronics product in the training set. For review-based QA systems like ours, it’s important to restrict the queries to a single item because otherwise the retriever would source reviews about products that are not related to a user’s query. For example, asking “Is the camera quality any good?” without a product filter could return reviews about phones, when the user might be asking about a specific laptop camera instead. By themselves, the ASIN values in our dataset are a bit cryptic, but we can decipher them with online tools like amazon ASIN or by simply appending the value of item_id to the www.amazon.com/dp/ URL. The following item ID corresponds to one of Amazon’s Fire tablets, so let’s use the retriever’s retrieve() method to ask if it’s any good for reading with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b44f614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id = \"B0074BW614\"\n",
    "query = \"Is it good for reading?\"\n",
    "retrieved_docs = es_retriever.retrieve(query=query, top_k=3, filters={\"item_id\":[item_id], \"split\":[\"train\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899d81d4",
   "metadata": {},
   "source": [
    "Here we’ve specified how many documents to return with the top_k argument and applied a filter on both the item_id and split keys that were included in the meta field of our documents. Each element of retrieved_docs is a Haystack Document object that is used to represent documents and includes the retriever’s query score along with other metadata. Let’s have a look at one of the retrieved documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "385d24fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Document: {'content': 'This is a gift to myself.  I have been a kindle user for 4 years and this is my third one.  I never thought I would want a fire for I mainly use it for book reading.  I decided to try the fire for when I travel I take my laptop, my phone and my iPod classic.  I love my iPod but watching movies on the plane with it can be challenging because it is so small. Laptops battery life is not as good as the Kindle.  So the Fire combines for me what I needed all three to do. So far so good.', 'content_type': 'text', 'score': 0.6857824513476455, 'meta': {'item_id': 'B0074BW614', 'question_id': '868e311275e26dbafe5af70774a300f3', 'split': 'train'}, 'embedding': None, 'id': '252e83e25d52df7311d597dc89eef9f6'}>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b3a7c0",
   "metadata": {},
   "source": [
    "In addition to the document’s text, we can see the score that Elasticsearch computed for its relevance to the query (larger scores imply a better match). Under the hood, Elasticsearch relies on Lucene for indexing and search, so by default it uses Lucene’s practical scoring function. You can find the nitty-gritty details behind the scoring function in the Elasticsearch documentation, but in brief terms it first filters the candidate documents by applying a Boolean test (does the document match the query?), and then applies a similarity metric that’s based on representing both the document and the query as vectors.\n",
    "\n",
    "Now that we have a way to retrieve relevant documents, the next thing we need is a way to extract answers from them. This is where the reader comes in, so let’s take a look at how we can load our MiniLM model in Haystack.\n",
    "\n",
    "## Initializing a reader\n",
    "In Haystack, there are two types of readers one can use to extract answers from a given context:\n",
    "\n",
    "#### FARMReader\n",
    "Based on deepset’s FARM framework (https://farm.deepset.ai/) for fine-tuning and deploying transformers. Compatible with models trained using Transformers and can load models directly from the Hugging Face Hub.\n",
    "\n",
    "#### TransformersReader\n",
    "Based on the QA pipeline from nlpt_pin01 Transformers. Suitable for running inference only.\n",
    "\n",
    "\n",
    "Although both readers handle a model’s weights in the same way, there are some differences in the way the predictions are converted to produce answers:\n",
    "\n",
    "In nlpt_pin01 Transformers, the QA pipeline normalizes the start and end logits with a softmax in each passage. This means that it is only meaningful to compare answer scores between answers extracted from the same passage, where the probabilities sum to 1. For example, an answer score of 0.9 from one passage is not necessarily better than a score of 0.8 in another. In FARM, the logits are not normalized, so inter-passage answers can be compared more easily.\n",
    "\n",
    "The TransformersReader sometimes predicts the same answer twice, but with different scores. This can happen in long contexts if the answer lies across two overlapping windows. In FARM, these duplicates are removed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617e6cfe",
   "metadata": {},
   "source": [
    "Since we will be fine-tuning the reader later in the chapter, we’ll use the FARMReader. As with nlpt_pin01 Transformers, to load the model we just need to specify the MiniLM checkpoint on the Hugging Face Hub along with some QA-specific arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e540ef4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CPU\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 0\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Could not find deepset/minilm-uncased-squad2 locally.\n",
      "INFO - haystack.modeling.model.language_model -  Looking on Transformers Model Hub (in local cache and online)...\n",
      "INFO - haystack.modeling.model.language_model -  Loaded deepset/minilm-uncased-squad2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.logger -  ML Logging is turned off. No parameters, metrics or artifacts will be logged to MLFlow.\n",
      "INFO - haystack.modeling.utils -  Using devices: CPU\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.infer -  Got ya 9 parallel workers to do inference ...\n",
      "INFO - haystack.modeling.infer -   0    0    0    0    0    0    0    0    0 \n",
      "INFO - haystack.modeling.infer -  /w\\  /w\\  /w\\  /w\\  /w\\  /w\\  /w\\  /|\\  /w\\\n",
      "INFO - haystack.modeling.infer -  /'\\  / \\  /'\\  /'\\  / \\  / \\  /'\\  /'\\  /'\\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from haystack.reader.farm import FARMReader\n",
    "\n",
    "model_ckpt = \"deepset/minilm-uncased-squad2\"\n",
    "max_seq_length, doc_stride = 384, 128\n",
    "\n",
    "reader = FARMReader(model_name_or_path=model_ckpt, progress_bar=False,\n",
    "                    max_seq_len=max_seq_length, doc_stride=doc_stride,\n",
    "                    return_no_answer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003175e9",
   "metadata": {},
   "source": [
    "It is also possible to fine-tune a reading comprehension model directly in nlpt_pin01 Transformers and then load it in TransformersReader to run inference. For details on how to do the fine-tuning step, see the question answering tutorial in the library’s documentation (https://oreil.ly/VkhIQ).\n",
    "\n",
    "In `FARMReader`, the behavior of the sliding window is controlled by the `same max_seq_length` and `doc_stride` arguments that we saw for the tokenizer. Here we’ve used the values from the MiniLM paper. To confirm, let’s now test the reader on our simple example from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "caaa73dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An MP3 is about 1 MB/minute, so about 6000 hours depending on file size.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86f6eb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'How much music can this hold?', 'no_ans_gap': 12.648093461990356, 'answers': [<Answer {'answer': '6000 hours', 'type': 'extractive', 'score': 0.5293049961328506, 'context': 'An MP3 is about 1 MB/minute, so about 6000 hours depending on file size.', 'offsets_in_document': [{'start': 38, 'end': 48}], 'offsets_in_context': [{'start': 38, 'end': 48}], 'document_id': 'e344757014e804eff50faa3ecf1c9c75', 'meta': {}}>]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilobustos/miniforge3/envs/tensorflow_m1/lib/python3.9/site-packages/haystack/modeling/model/prediction_head.py:480: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  start_indices = flat_sorted_indices // max_seq_len\n"
     ]
    }
   ],
   "source": [
    "print(reader.predict_on_texts(question=question, texts=[context], top_k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "faa73d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is Python?', 'no_ans_gap': 13.30579799413681, 'answers': [<Answer {'answer': 'a language programming', 'type': 'extractive', 'score': 0.8164567947387695, 'context': 'Python is a language programming or it is also a huge snake. Both of them are good answers', 'offsets_in_document': [{'start': 10, 'end': 32}], 'offsets_in_context': [{'start': 10, 'end': 32}], 'document_id': '9e6ec4d0ed34ef004f375bdf2752b734', 'meta': {}}>]}\n"
     ]
    }
   ],
   "source": [
    "print(reader.predict_on_texts(question='What is Python?', texts=['Python is a language programming or it is also a huge snake. Both of them are good answers'], top_k=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a93fc08",
   "metadata": {},
   "source": [
    "Great, the reader appears to be working as expected—so next, let’s tie together all our components using one of Haystack’s pipelines.\n",
    "\n",
    "## Putting all together\n",
    "\n",
    "Haystack provides a Pipeline abstraction that allows us to combine retrievers, readers, and other components together as a graph that can be easily customized for each use case. There are also predefined pipelines analogous to those in nlpt_pin01 Transformers, but specialized for QA systems. In our case, we’re interested in extracting answers, so we’ll use the ExtractiveQAPipeline, which takes a single retriever-reader pair as its arguments:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f29dc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - haystack -  Object 'ExtractiveQAPipeline' is imported through a deprecated path. Please check out the docs for the new import path.\n"
     ]
    }
   ],
   "source": [
    "from haystack.pipeline import ExtractiveQAPipeline\n",
    "\n",
    "pipe = ExtractiveQAPipeline(reader, es_retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917bf690",
   "metadata": {},
   "source": [
    "Each Pipeline has a `run()` method that specifies how the query flow should be executed. For the `ExtractiveQAPipeline` we just need to pass the query, the number of documents to retrieve with `top_k_retriever`, and the number of answers to extract from these documents with `top_k_reader`. In our case, we also need to specify a filter over the `item ID`, which can be done using the filters argument as we did with the retriever earlier. Let’s run a simple example using our question about the Amazon Fire tablet again, but this time returning the extracted answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6512a5bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is it good for reading?'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ed4e027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B0074BW614'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "96a541e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is it good for reading? \n",
      "\n",
      "Answer 1: it is great for reading books when no light is available\n",
      "Review snippet: ...ecoming addicted to hers! Our son LOVES it and it is great for reading books when no light is available. Amazing sound but I suggest good headphones t...\n",
      "\n",
      "\n",
      "\n",
      "Answer 2: I mainly use it for book reading\n",
      "Review snippet: ... is my third one.  I never thought I would want a fire for I mainly use it for book reading.  I decided to try the fire for when I travel I take my la...\n",
      "\n",
      "\n",
      "\n",
      "Answer 3: \n",
      "Review snippet: ...None...\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_answers = 3\n",
    "preds = pipe.run(query=query, params={\n",
    "    \"Retriever\": {\"top_k\": 3, \"filters\": {\"item_id\": [item_id], \"split\": [\"train\"]}}, \n",
    "    \"Reader\": {\"top_k\": n_answers}\n",
    "})\n",
    "\n",
    "print(f\"Question: {preds['query']} \\n\")\n",
    "for idx in range(n_answers):\n",
    "    print(f\"Answer {idx+1}: {preds['answers'][idx].answer}\")\n",
    "    print(f\"Review snippet: ...{preds['answers'][idx].context}...\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f75efa8",
   "metadata": {},
   "source": [
    "Great, we now have an end-to-end QA system for Amazon product reviews! This is a good start, but notice that the second and third answers are closer to what the question is actually asking. To do better, we’ll need some metrics to quantify the performance of the retriever and reader. We’ll take a look at that next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ed24b2",
   "metadata": {},
   "source": [
    "## Improving our QA pipeline\n",
    "\n",
    "Although much of the recent research on QA has focused on improving reading comprehension models, in practice it doesn’t matter how good your reader is if the retriever can’t find the relevant documents in the first place! In particular, the retriever sets an upper bound on the performance of the whole QA system, so it’s important to make sure it’s doing a good job. With this in mind, let’s start by introducing some common metrics to evaluate the retriever so that we can compare the performance of sparse and dense representations.\n",
    "\n",
    "### Evaluating Retriever\n",
    "\n",
    "In Haystack, there are two ways to evaluate retrievers:\n",
    "\n",
    "Use the retriever’s in-built eval() method. This can be used for both open- and closed-domain QA, but not for datasets like SubjQA where each document is paired with a single product and we need to filter by product ID for every query.\n",
    "\n",
    "Build a custom Pipeline that combines a retriever with the EvalRetriever class. This enables the implementation of custom metrics and query flows.\n",
    "\n",
    "Since we need to evaluate the recall per product and then aggregate across all products, we’ll opt for the second approach. Each node in the Pipeline graph represents a class that takes some inputs and produces some outputs via a run() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df6c83c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b567aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b71d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f150ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
